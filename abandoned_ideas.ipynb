{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# === RDKit hierarchicial clustering ===\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.ML.Cluster import Butina\n",
    "\n",
    "def ClusterFps(fps,cutoff=0.2):\n",
    "\n",
    "    # first generate the distance matrix:\n",
    "    dists = []\n",
    "    nfps = len(fps)\n",
    "    for i in range(1,nfps):\n",
    "        sims = DataStructs.BulkTanimotoSimilarity(fps[i],fps[:i]) # calculate similarity between each molecule fingerprint\n",
    "        dists.extend([1-x for x in sims]) # similarity to distance\n",
    "\n",
    "    # now cluster the data:\n",
    "    cs = Butina.ClusterData(dists,nfps,cutoff,isDistData=True)\n",
    "    return cs\n",
    "\n",
    "\n",
    "fps = [AllChem.GetMorganFingerprintAsBitVect(x,2,1024) for x in molecules]\n",
    "clusters = ClusterFps(fps, 0.6)\n",
    "\n",
    "clusters"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_reactions_with_numpy(file_path:str): # -> List[str]:\n",
    "    # This was a nice idea but it does a lot to mess up the formatting of the file when we convert the array back a string\n",
    "    # might be fixable\n",
    "    \"\"\"Extracts the reactions from a .rdf file and returns them as a list of strings.\"\"\"\n",
    "\n",
    "    section_seperator = \"$RXN\"\n",
    "    rxn_blocks = [] # this will store each extracted reaction block\n",
    "\n",
    "    # === Parse File ===\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.read().splitlines() # read all lines into a list\n",
    "\n",
    "    lines = np.array(lines) # convert lines list to a numpy array\n",
    "    rxn_block_idxs = np.where(lines == section_seperator) # find all instances $RXN and save as np array\n",
    "    num_reactions = np.shape(rxn_block_idxs)[1] - 1\n",
    "\n",
    "    # === Extract Reactions ===\n",
    "    for ii in range(num_reactions):\n",
    "        # iterate over the found $RXN tags\n",
    "        rxn_block_start = rxn_block_idxs[0][ii] # Get index of block start - each block starts on line with $RXN\n",
    "        rxn_block_end = rxn_block_idxs[0][ii+1] - 1 # Get index of block end - block ends one line before the next $RXN\n",
    "        \n",
    "        rxn_block = lines[rxn_block_start:rxn_block_end] # retreive all lines of this reaction\n",
    "        rxn_block = np.array2string(rxn_block, \n",
    "                                    separator=\"\\n\")\n",
    "                                    #formatter={\"str_kind\": lambda x: \"%s \" % x}) # convert array back to a string\n",
    "        \n",
    "        rxn_block = rxn_block[1:-1] # remove leading and trailing \"[\" and \"]\"\n",
    "        rxn_blocks.append(rxn_block)\n",
    "    \n",
    "    return rxn_blocks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# high_dpi = True\n",
    "# # adjust basic settings for high DPI screen\n",
    "# if high_dpi:\n",
    "#     plt.rcParams['font.size'] = 14 # set the font size for all plots\n",
    "#     plt.rcParams['figure.figsize'] = (10,10) # set the figure size for all plots"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_sensor_data(pccr_file_path: str,\n",
    "                        sensor_name: str,\n",
    "                        stop_after_first: bool = False): # -> pd.DataFrame: # adding the return type hints messes up the syntax highlightning in notebooks in VSCode\n",
    "\n",
    "    \"\"\"Extracts values and timestamps from the given pcrr file and sensor.\n",
    "    pccr_file_path: Path to the pcrr file\n",
    "    sensor_name: The name of the sensor to read data for\n",
    "    stop_after_first: Defaults to False. Set True to stop parsing the document after the first instance of the sensor is found.\n",
    "    \"\"\"\n",
    "    \n",
    "    # === Check for correct file type ===\n",
    "    pccr_file_ext = \".pcrr\"\n",
    "    if not check_file_extension(pccr_file_path, pccr_file_ext):\n",
    "        print(f\"{pccr_file_path} is not a .pccr file, aborting.\")\n",
    "        return None\n",
    "\n",
    "    sensor_data_value = []\n",
    "    sensor_data_timestamp = []\n",
    "\n",
    "    # === XML Parsing ===\n",
    "    doc = pulldom.parse(pcrr_file_path) # these xml files are very large, use pulldom to extract the parts we need\n",
    "    for event, node in doc:\n",
    "        if event == pulldom.START_ELEMENT:\n",
    "            # could extend this here by iterating over multiple sensors, meaning we would only have to process the xml file once rather than n-sensor times\n",
    "            if node.tagName == \"sensor_data\" and node.getAttribute(\"name\") == sensor_name: # find the sensor_data tag with the name attribute of the sensor we're interested in\n",
    "                doc.expandNode(node) # expand the node so we can parse it with elementree and xpath\n",
    "                root = ET.fromstring(node.toxml()) # load the xml into elementree\n",
    "\n",
    "                # find all instances of the sensor_data_record tag within this node, record the values and timestamps\n",
    "                sensor_data_value += [x.text for x in root.findall(\"./sensor_data_record/value\")] \n",
    "                sensor_data_timestamp += [x.text for x in root.findall(\"./sensor_data_record/timestamp\")]\n",
    "\n",
    "                if stop_after_first: # sensor data is present in mutliple nodes. If set, stop parsing after the first instance of the target tag, useful for debugging/demo purposes as it is otherwise quite slow because of the large file size\n",
    "                    break\n",
    "\n",
    "    # === Create Dataframe ===\n",
    "    df = pd.DataFrame(data={sensor_name: sensor_data_value, \"timestamp\": sensor_data_timestamp}) #convert to a dataframe\n",
    "    df = df.astype({sensor_name: float, \"timestamp\": \"datetime64[ns]\"}) # set datatypes\n",
    "    df.set_index(\"timestamp\", inplace=True) # set the timestamp as the index\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}